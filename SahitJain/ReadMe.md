Dataset:

{
	"answers":["A corporation is a company or group of people authorized to act as a single entity and recognized as such in law."],
	"passages":[
		{
			"is_selected":0,
			"url":"http:\/\/www.wisegeek.com\/what-is-a-corporation.htm",
			"passage_text":"A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders, either directly or indirectly."},
		...
		}],
	"query":". what is a corporation?",
	"query_id":1102432,
	"query_type":"DESCRIPTION",
	"wellFormedAnswers":"[]"
}

Columns:
1.	“answer”: Contains the exact answer for the query, written by a human after annotating through the passages.
2.	“passages”: For every query human annotators chose from 10 passages selected from the top 10 search results of the query on Bing Search engine.
It Contains 3 sub fields:-
a.	“is_selected”: If the passages was used in the answer or not.
b.	“passage_text”: Actual passage content.
c.	“url”: url of where the passage was retrieved from.
3.	“query”: Contains the actual question.
4.	“query_id”: A unique identifier for each query.
5.	“query_type”: Describes the type of query.
6.	“wellFormedAnswer”: For some queries the answers lacked context and/or proper grammar. Such answers were rewritten as well-formed answers.

Tasks Performed:
	
Each member of the team picked a unique dataset and automatically generated Semantic Information Availability (SIA) scores for each <Query, Sentence> pair. By either developing their own models or using existing pretrained ones.

I started by downloading the dataset from python package ‘dataset’ by hugging face. I used pandas DataFrames to handle my data. 
	
For reasonable compute times, I worked on just 10% of the Validation data, which gave out around 100,000 <Query, Sentence> pairs. The code can handle the complete dataset though.

In the preprocessing phase, I removed the unwanted columns i.e. ‘query_type’, ‘query_id’.
 And replaced the answers with wellFormedAnswer for the queries that had a wellFormedAnswer available. And extracted 10 passages for each query from the passage column. Followed by generating the <Query, Answer> Pair. 

Some queries didn’t have any answers that were filled with NaN.

For the task of generating I tried various pre-trained STS models:
•	Custom BERT based STS model with a LR layer to generate STS scores, made by teammate Mihir for one of his earlier projects.
https://colab.research.google.com/drive/1XpP3sb7k-VFrc42XR34gaMNbkV4E95cx?usp=sharing
•	Web STS BERT base.
https://github.com/AndriyMulyar/semantic-text-similarity/tree/master/semantic_text_similarity
•	DISTILBERT base pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.
•	ROBERTA large pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.
•	ROBERTA base pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.
•	BERT base pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.

For the last four models, the score was generated by calculating the cosine similarity of the sentence and <Query, Answer> embeddings returned by the models.
			https://github.com/UKPLab/sentence-transformers
I went with ROBERTA base model since that has the best tradeoff between accuracy and compute time.
To Execute the code, just run the python notebook, installing and importing the required packages.

You’ll have to mount drive and load this file in you Google Drive:

Google Drive File link:
https://drive.google.com/drive/folders/1bdbvKGo9VWEY2NIpPBngGu_lEklULhNz?usp=sharing

Combined Dataset can be found at out Project GitHub Repo:
https://github.com/JainSahit/NLP576-SIA
Columns:
1.	“answer”: Contains the exact answer for the query, written by a human after annotating through the passages.
2.	“passages”: For every query human annotators chose from 10 passages selected from the top 10 search results of the query on Bing Search engine.
It Contains 3 sub fields:-
a.	“is_selected”: If the passages was used in the answer or not.
b.	“passage_text”: Actual passage content.
c.	“url”: url of where the passage was retrieved from.
3.	“query”: Contains the actual question.
4.	“query_id”: A unique identifier for each query.
5.	“query_type”: Describes the type of query.
6.	“wellFormedAnswer”: For some queries the answers lacked context and/or proper grammar. Such answers were rewritten as well-formed answers.


Tasks Performed:
	
Each member of the team picked a unique dataset and automatically generated Semantic Information Availability (SIA) scores for each <Query, Sentence> pair. By either developing their own models or using existing pretrained ones.

I started by downloading the dataset from python package ‘dataset’ by hugging face. I used pandas DataFrames to handle my data. 
	
For reasonable compute times, I worked on just 10% of the Validation data, which gave out around 100,000 <Query, Sentence> pairs. The code can handle the complete dataset though.

In the preprocessing phase, I removed the unwanted columns i.e. ‘query_type’, ‘query_id’.
 And replaced the answers with wellFormedAnswer for the queries that had a wellFormedAnswer available. And extracted 10 passages for each query from the passage column. Followed by generating the <Query, Answer> Pair. 

Some queries didn’t have any answers that were filled with NaN.

For the task of generating I tried various pre-trained STS models:
•	Custom BERT based STS model with a LR layer to generate STS scores, made by teammate Mihir for one of his earlier projects.
https://colab.research.google.com/drive/1XpP3sb7k-VFrc42XR34gaMNbkV4E95cx?usp=sharing
•	Web STS BERT base.
https://github.com/AndriyMulyar/semantic-text-similarity/tree/master/semantic_text_similarity
•	DISTILBERT base pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.
•	ROBERTA large pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.
•	ROBERTA base pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.
•	BERT base pretrained on NLI dataset followed by STS B dataset pooling on mean tokens.

For the last four models, the score was generated by calculating the cosine similarity of the sentence and <Query, Answer> embeddings returned by the models.
			https://github.com/UKPLab/sentence-transformers
I went with ROBERTA base model since that has the best tradeoff between accuracy and compute time.
To Execute the code, just run the python notebook, installing and importing the required packages.

You’ll have to mount drive and load this file in you Google Drive:

Google Drive File link:
https://drive.google.com/drive/folders/1bdbvKGo9VWEY2NIpPBngGu_lEklULhNz?usp=sharing

Combined Dataset can be found at out Project GitHub Repo:
https://github.com/JainSahit/NLP576-SIA

