{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyserini_and_Data_PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9-y3Wxp2gHI"
      },
      "source": [
        "## **Mounting the Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu5s8AtT4xZA",
        "outputId": "2de776d1-edac-4798-c93e-03aa9978c08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT0uILsko5RO"
      },
      "source": [
        "## **Pyserini Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSo-KR088faq",
        "outputId": "f106fbd7-7691-4c67-c544-598b1efb2dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "!pip install pyserini==0.9.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyserini==0.9.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/ae/cf4a44fbeb6e1f947dfde40fa0142de33ae263333bda26edca914be6ba1b/pyserini-0.9.4.0-py3-none-any.whl (60.4MB)\n",
            "\u001b[K     |████████████████████████████████| 60.4MB 67kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.4.0) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.4.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.4.0) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.4.0) (1.1.2)\n",
            "Collecting pyjnius\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/50/098cb5fb76fb7c7d99d403226a2a63dcbfb5c129b71b7d0f5200b05de1f0/pyjnius-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.4.0) (0.29.21)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyserini==0.9.4.0) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyserini==0.9.4.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyserini==0.9.4.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pyjnius->pyserini==0.9.4.0) (1.15.0)\n",
            "Installing collected packages: pyjnius, pyserini\n",
            "Successfully installed pyjnius-1.3.0 pyserini-0.9.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzi82ZpV8oYL",
        "outputId": "3b39a7ae-21f1-40a6-dc5a-5214cb77a9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-fMAsjX9DBr",
        "outputId": "58472b6f-8d22-429a-c96c-46ae73ba6f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"     #set environment variable\n",
        "  !java -version     #check java version\n",
        "install_java()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.8\" 2020-07-14\n",
            "OpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn7-8r7d1O-9"
      },
      "source": [
        "# **Clone the Pyserini Repository**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-ETVO789F2j",
        "outputId": "a783c310-4a0e-47b3-a8ba-f273149309cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!git clone https://github.com/castorini/pyserini.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyserini'...\n",
            "remote: Enumerating objects: 1267, done.\u001b[K\n",
            "remote: Total 1267 (delta 0), reused 0 (delta 0), pack-reused 1267\u001b[K\n",
            "Receiving objects: 100% (1267/1267), 378.78 KiB | 2.85 MiB/s, done.\n",
            "Resolving deltas: 100% (810/810), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_gP9Vy59aSq",
        "outputId": "2ad098f9-abdd-4bce-f5c4-3079d6c327e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#%cd pyserini\n",
        "#!git clone https://github.com/castorini/anserini-tools.git\n",
        "%cd /content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxBhMpah9g7G"
      },
      "source": [
        "sentences= []\n",
        "index=[]\n",
        "i=1\n",
        "corpus_sentences=open('/content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/QASC_Corpus.txt','r')\n",
        "for line in corpus_sentences.readlines():\n",
        "    lin = line.replace('\\n', ' ')\n",
        "    index.append(i)\n",
        "    sentences.append(lin)\n",
        "    i=i+1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PQ86MA1_JG5",
        "outputId": "e3ea8fc5-85bc-49b7-a053-3530a6aee04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(index[100000])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULObrZK_RI9"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(list(zip(index, sentences)), columns=['index','paragraph'])\n",
        "df.to_csv('/content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini/pyserini/collections/document_passages.tsv', header=False, index=False, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1xUMuIzpEr-"
      },
      "source": [
        "**Loading input Corpus dataset in TSV format and converting it into JSONL with each file containing max 1000000(1M) Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sVQvJVKAhyK"
      },
      "source": [
        "df_data= pd.read_csv('/content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini/pyserini/collections/document_passages.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNg-bXrgBHUP",
        "outputId": "69e2dd39-e477-4771-b925-39359513be66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python pyserini/anserini-tools/scripts/msmarco/convert_collection_to_jsonl.py \\\n",
        " --collection-path pyserini/collections/document_passages.tsv \\\n",
        " --output-folder pyserini/collections/collection_jsonl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting collection...\n",
            "Converted 0 docs, writing into file 1\n",
            "Converted 100,000 docs, writing into file 1\n",
            "Converted 200,000 docs, writing into file 1\n",
            "Converted 300,000 docs, writing into file 1\n",
            "Converted 400,000 docs, writing into file 1\n",
            "Converted 500,000 docs, writing into file 1\n",
            "Converted 600,000 docs, writing into file 1\n",
            "Converted 700,000 docs, writing into file 1\n",
            "Converted 800,000 docs, writing into file 1\n",
            "Converted 900,000 docs, writing into file 1\n",
            "Converted 1,000,000 docs, writing into file 2\n",
            "Converted 1,100,000 docs, writing into file 2\n",
            "Converted 1,200,000 docs, writing into file 2\n",
            "Converted 1,300,000 docs, writing into file 2\n",
            "Converted 1,400,000 docs, writing into file 2\n",
            "Converted 1,500,000 docs, writing into file 2\n",
            "Converted 1,600,000 docs, writing into file 2\n",
            "Converted 1,700,000 docs, writing into file 2\n",
            "Converted 1,800,000 docs, writing into file 2\n",
            "Converted 1,900,000 docs, writing into file 2\n",
            "Converted 2,000,000 docs, writing into file 3\n",
            "Converted 2,100,000 docs, writing into file 3\n",
            "Converted 2,200,000 docs, writing into file 3\n",
            "Converted 2,300,000 docs, writing into file 3\n",
            "Converted 2,400,000 docs, writing into file 3\n",
            "Converted 2,500,000 docs, writing into file 3\n",
            "Converted 2,600,000 docs, writing into file 3\n",
            "Converted 2,700,000 docs, writing into file 3\n",
            "Converted 2,800,000 docs, writing into file 3\n",
            "Converted 2,900,000 docs, writing into file 3\n",
            "Converted 3,000,000 docs, writing into file 4\n",
            "Converted 3,100,000 docs, writing into file 4\n",
            "Converted 3,200,000 docs, writing into file 4\n",
            "Converted 3,300,000 docs, writing into file 4\n",
            "Converted 3,400,000 docs, writing into file 4\n",
            "Converted 3,500,000 docs, writing into file 4\n",
            "Converted 3,600,000 docs, writing into file 4\n",
            "Converted 3,700,000 docs, writing into file 4\n",
            "Converted 3,800,000 docs, writing into file 4\n",
            "Converted 3,900,000 docs, writing into file 4\n",
            "Converted 4,000,000 docs, writing into file 5\n",
            "Converted 4,100,000 docs, writing into file 5\n",
            "Converted 4,200,000 docs, writing into file 5\n",
            "Converted 4,300,000 docs, writing into file 5\n",
            "Converted 4,400,000 docs, writing into file 5\n",
            "Converted 4,500,000 docs, writing into file 5\n",
            "Converted 4,600,000 docs, writing into file 5\n",
            "Converted 4,700,000 docs, writing into file 5\n",
            "Converted 4,800,000 docs, writing into file 5\n",
            "Converted 4,900,000 docs, writing into file 5\n",
            "Converted 5,000,000 docs, writing into file 6\n",
            "Converted 5,100,000 docs, writing into file 6\n",
            "Converted 5,200,000 docs, writing into file 6\n",
            "Converted 5,300,000 docs, writing into file 6\n",
            "Converted 5,400,000 docs, writing into file 6\n",
            "Converted 5,500,000 docs, writing into file 6\n",
            "Converted 5,600,000 docs, writing into file 6\n",
            "Converted 5,700,000 docs, writing into file 6\n",
            "Converted 5,800,000 docs, writing into file 6\n",
            "Converted 5,900,000 docs, writing into file 6\n",
            "Converted 6,000,000 docs, writing into file 7\n",
            "Converted 6,100,000 docs, writing into file 7\n",
            "Converted 6,200,000 docs, writing into file 7\n",
            "Converted 6,300,000 docs, writing into file 7\n",
            "Converted 6,400,000 docs, writing into file 7\n",
            "Converted 6,500,000 docs, writing into file 7\n",
            "Converted 6,600,000 docs, writing into file 7\n",
            "Converted 6,700,000 docs, writing into file 7\n",
            "Converted 6,800,000 docs, writing into file 7\n",
            "Converted 6,900,000 docs, writing into file 7\n",
            "Converted 7,000,000 docs, writing into file 8\n",
            "Converted 7,100,000 docs, writing into file 8\n",
            "Converted 7,200,000 docs, writing into file 8\n",
            "Converted 7,300,000 docs, writing into file 8\n",
            "Converted 7,400,000 docs, writing into file 8\n",
            "Converted 7,500,000 docs, writing into file 8\n",
            "Converted 7,600,000 docs, writing into file 8\n",
            "Converted 7,700,000 docs, writing into file 8\n",
            "Converted 7,800,000 docs, writing into file 8\n",
            "Converted 7,900,000 docs, writing into file 8\n",
            "Converted 8,000,000 docs, writing into file 9\n",
            "Converted 8,100,000 docs, writing into file 9\n",
            "Converted 8,200,000 docs, writing into file 9\n",
            "Converted 8,300,000 docs, writing into file 9\n",
            "Converted 8,400,000 docs, writing into file 9\n",
            "Converted 8,500,000 docs, writing into file 9\n",
            "Converted 8,600,000 docs, writing into file 9\n",
            "Converted 8,700,000 docs, writing into file 9\n",
            "Converted 8,800,000 docs, writing into file 9\n",
            "Converted 8,900,000 docs, writing into file 9\n",
            "Converted 9,000,000 docs, writing into file 10\n",
            "Converted 9,100,000 docs, writing into file 10\n",
            "Converted 9,200,000 docs, writing into file 10\n",
            "Converted 9,300,000 docs, writing into file 10\n",
            "Converted 9,400,000 docs, writing into file 10\n",
            "Converted 9,500,000 docs, writing into file 10\n",
            "Converted 9,600,000 docs, writing into file 10\n",
            "Converted 9,700,000 docs, writing into file 10\n",
            "Converted 9,800,000 docs, writing into file 10\n",
            "Converted 9,900,000 docs, writing into file 10\n",
            "Converted 10,000,000 docs, writing into file 11\n",
            "Converted 10,100,000 docs, writing into file 11\n",
            "Converted 10,200,000 docs, writing into file 11\n",
            "Converted 10,300,000 docs, writing into file 11\n",
            "Converted 10,400,000 docs, writing into file 11\n",
            "Converted 10,500,000 docs, writing into file 11\n",
            "Converted 10,600,000 docs, writing into file 11\n",
            "Converted 10,700,000 docs, writing into file 11\n",
            "Converted 10,800,000 docs, writing into file 11\n",
            "Converted 10,900,000 docs, writing into file 11\n",
            "Converted 11,000,000 docs, writing into file 12\n",
            "Converted 11,100,000 docs, writing into file 12\n",
            "Converted 11,200,000 docs, writing into file 12\n",
            "Converted 11,300,000 docs, writing into file 12\n",
            "Converted 11,400,000 docs, writing into file 12\n",
            "Converted 11,500,000 docs, writing into file 12\n",
            "Converted 11,600,000 docs, writing into file 12\n",
            "Converted 11,700,000 docs, writing into file 12\n",
            "Converted 11,800,000 docs, writing into file 12\n",
            "Converted 11,900,000 docs, writing into file 12\n",
            "Converted 12,000,000 docs, writing into file 13\n",
            "Converted 12,100,000 docs, writing into file 13\n",
            "Converted 12,200,000 docs, writing into file 13\n",
            "Converted 12,300,000 docs, writing into file 13\n",
            "Converted 12,400,000 docs, writing into file 13\n",
            "Converted 12,500,000 docs, writing into file 13\n",
            "Converted 12,600,000 docs, writing into file 13\n",
            "Converted 12,700,000 docs, writing into file 13\n",
            "Converted 12,800,000 docs, writing into file 13\n",
            "Converted 12,900,000 docs, writing into file 13\n",
            "Converted 13,000,000 docs, writing into file 14\n",
            "Converted 13,100,000 docs, writing into file 14\n",
            "Converted 13,200,000 docs, writing into file 14\n",
            "Converted 13,300,000 docs, writing into file 14\n",
            "Converted 13,400,000 docs, writing into file 14\n",
            "Converted 13,500,000 docs, writing into file 14\n",
            "Converted 13,600,000 docs, writing into file 14\n",
            "Converted 13,700,000 docs, writing into file 14\n",
            "Converted 13,800,000 docs, writing into file 14\n",
            "Converted 13,900,000 docs, writing into file 14\n",
            "Converted 14,000,000 docs, writing into file 15\n",
            "Converted 14,100,000 docs, writing into file 15\n",
            "Converted 14,200,000 docs, writing into file 15\n",
            "Converted 14,300,000 docs, writing into file 15\n",
            "Converted 14,400,000 docs, writing into file 15\n",
            "Converted 14,500,000 docs, writing into file 15\n",
            "Converted 14,600,000 docs, writing into file 15\n",
            "Converted 14,700,000 docs, writing into file 15\n",
            "Converted 14,800,000 docs, writing into file 15\n",
            "Converted 14,900,000 docs, writing into file 15\n",
            "Converted 15,000,000 docs, writing into file 16\n",
            "Converted 15,100,000 docs, writing into file 16\n",
            "Converted 15,200,000 docs, writing into file 16\n",
            "Converted 15,300,000 docs, writing into file 16\n",
            "Converted 15,400,000 docs, writing into file 16\n",
            "Converted 15,500,000 docs, writing into file 16\n",
            "Converted 15,600,000 docs, writing into file 16\n",
            "Converted 15,700,000 docs, writing into file 16\n",
            "Converted 15,800,000 docs, writing into file 16\n",
            "Converted 15,900,000 docs, writing into file 16\n",
            "Converted 16,000,000 docs, writing into file 17\n",
            "Converted 16,100,000 docs, writing into file 17\n",
            "Converted 16,200,000 docs, writing into file 17\n",
            "Converted 16,300,000 docs, writing into file 17\n",
            "Converted 16,400,000 docs, writing into file 17\n",
            "Converted 16,500,000 docs, writing into file 17\n",
            "Converted 16,600,000 docs, writing into file 17\n",
            "Converted 16,700,000 docs, writing into file 17\n",
            "Converted 16,800,000 docs, writing into file 17\n",
            "Converted 16,900,000 docs, writing into file 17\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZZ8bzS5pcun"
      },
      "source": [
        "**Creating JSON collections for the corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz_ubIy1DAdx",
        "outputId": "00153a84-fd55-43e6-cbaa-a5d00479441b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python -m pyserini.index -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
        " -threads 9 -input pyserini/collections/collection_jsonl \\\n",
        " -index pyserini/indexes/lucene-index-qasc -storePositions -storeDocvectors -storeRaw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2020-10-21 03:32:33,608 INFO  [main] index.IndexCollection (IndexCollection.java:636) - Setting log level to INFO\n",
            "2020-10-21 03:32:33,611 INFO  [main] index.IndexCollection (IndexCollection.java:639) - Starting indexer...\n",
            "2020-10-21 03:32:33,611 INFO  [main] index.IndexCollection (IndexCollection.java:640) - ============ Loading Parameters ============\n",
            "2020-10-21 03:32:33,612 INFO  [main] index.IndexCollection (IndexCollection.java:641) - DocumentCollection path: pyserini/collections/collection_jsonl\n",
            "2020-10-21 03:32:33,612 INFO  [main] index.IndexCollection (IndexCollection.java:642) - CollectionClass: JsonCollection\n",
            "2020-10-21 03:32:33,613 INFO  [main] index.IndexCollection (IndexCollection.java:643) - Generator: DefaultLuceneDocumentGenerator\n",
            "2020-10-21 03:32:33,613 INFO  [main] index.IndexCollection (IndexCollection.java:644) - Threads: 9\n",
            "2020-10-21 03:32:33,614 INFO  [main] index.IndexCollection (IndexCollection.java:645) - Stemmer: porter\n",
            "2020-10-21 03:32:33,614 INFO  [main] index.IndexCollection (IndexCollection.java:646) - Keep stopwords? false\n",
            "2020-10-21 03:32:33,614 INFO  [main] index.IndexCollection (IndexCollection.java:647) - Stopwords:  null\n",
            "2020-10-21 03:32:33,615 INFO  [main] index.IndexCollection (IndexCollection.java:648) - Store positions? true\n",
            "2020-10-21 03:32:33,615 INFO  [main] index.IndexCollection (IndexCollection.java:649) - Store docvectors? true\n",
            "2020-10-21 03:32:33,616 INFO  [main] index.IndexCollection (IndexCollection.java:650) - Store document \"contents\" field? false\n",
            "2020-10-21 03:32:33,616 INFO  [main] index.IndexCollection (IndexCollection.java:651) - Store document \"raw\" field? true\n",
            "2020-10-21 03:32:33,616 INFO  [main] index.IndexCollection (IndexCollection.java:652) - Optimize (merge segments)? false\n",
            "2020-10-21 03:32:33,617 INFO  [main] index.IndexCollection (IndexCollection.java:653) - Whitelist: null\n",
            "2020-10-21 03:32:33,617 INFO  [main] index.IndexCollection (IndexCollection.java:673) - Directly building Lucene indexes...\n",
            "2020-10-21 03:32:33,617 INFO  [main] index.IndexCollection (IndexCollection.java:674) - Index path: pyserini/indexes/lucene-index-qasc\n",
            "2020-10-21 03:32:33,631 INFO  [main] index.IndexCollection (IndexCollection.java:723) - ============ Indexing Collection ============\n",
            "2020-10-21 03:32:33,838 INFO  [main] index.IndexCollection (IndexCollection.java:784) - Thread pool with 9 threads initialized.\n",
            "2020-10-21 03:32:33,838 INFO  [main] index.IndexCollection (IndexCollection.java:786) - Initializing collection in pyserini/collections/collection_jsonl\n",
            "2020-10-21 03:32:33,850 INFO  [main] index.IndexCollection (IndexCollection.java:789) - 17 files found\n",
            "2020-10-21 03:32:33,850 INFO  [main] index.IndexCollection (IndexCollection.java:790) - Starting to index...\n",
            "2020-10-21 03:33:33,873 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 0.00% of files completed, 1,590,000 documents indexed\n",
            "2020-10-21 03:34:33,874 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 0.00% of files completed, 3,910,000 documents indexed\n",
            "2020-10-21 03:35:33,877 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 0.00% of files completed, 6,220,000 documents indexed\n",
            "2020-10-21 03:36:33,880 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 0.00% of files completed, 8,490,000 documents indexed\n",
            "2020-10-21 03:36:41,408 DEBUG [pool-2-thread-9] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs16.json: 987130 docs added.\n",
            "2020-10-21 03:36:45,854 DEBUG [pool-2-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs01.json: 1000000 docs added.\n",
            "2020-10-21 03:36:46,139 DEBUG [pool-2-thread-6] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs05.json: 1000000 docs added.\n",
            "2020-10-21 03:36:46,920 DEBUG [pool-2-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs02.json: 1000000 docs added.\n",
            "2020-10-21 03:36:46,991 DEBUG [pool-2-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs03.json: 1000000 docs added.\n",
            "2020-10-21 03:36:47,599 DEBUG [pool-2-thread-8] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs07.json: 1000000 docs added.\n",
            "2020-10-21 03:36:47,859 DEBUG [pool-2-thread-5] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs04.json: 1000000 docs added.\n",
            "2020-10-21 03:36:47,862 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs00.json: 1000000 docs added.\n",
            "2020-10-21 03:36:48,115 DEBUG [pool-2-thread-7] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs06.json: 1000000 docs added.\n",
            "2020-10-21 03:37:33,882 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 52.94% of files completed, 10,597,130 documents indexed\n",
            "2020-10-21 03:38:33,886 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 52.94% of files completed, 12,837,130 documents indexed\n",
            "2020-10-21 03:39:33,888 INFO  [main] index.IndexCollection (IndexCollection.java:810) - 52.94% of files completed, 15,047,130 documents indexed\n",
            "2020-10-21 03:40:24,110 DEBUG [pool-2-thread-9] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs15.json: 1000000 docs added.\n",
            "2020-10-21 03:40:28,483 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs12.json: 1000000 docs added.\n",
            "2020-10-21 03:40:29,171 DEBUG [pool-2-thread-8] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs10.json: 1000000 docs added.\n",
            "2020-10-21 03:40:29,485 DEBUG [pool-2-thread-6] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs14.json: 1000000 docs added.\n",
            "2020-10-21 03:40:29,682 DEBUG [pool-2-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs13.json: 1000000 docs added.\n",
            "2020-10-21 03:40:29,705 DEBUG [pool-2-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs08.json: 1000000 docs added.\n",
            "2020-10-21 03:40:29,771 DEBUG [pool-2-thread-5] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs11.json: 1000000 docs added.\n",
            "2020-10-21 03:40:29,931 DEBUG [pool-2-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:238) - collection_jsonl/docs09.json: 1000000 docs added.\n",
            "2020-10-21 03:42:11,077 INFO  [main] index.IndexCollection (IndexCollection.java:874) - Indexing Complete! 16,987,130 documents indexed\n",
            "2020-10-21 03:42:11,077 INFO  [main] index.IndexCollection (IndexCollection.java:875) - ============ Final Counter Values ============\n",
            "2020-10-21 03:42:11,077 INFO  [main] index.IndexCollection (IndexCollection.java:876) - indexed:       16,987,130\n",
            "2020-10-21 03:42:11,078 INFO  [main] index.IndexCollection (IndexCollection.java:877) - unindexable:            0\n",
            "2020-10-21 03:42:11,078 INFO  [main] index.IndexCollection (IndexCollection.java:878) - empty:                  0\n",
            "2020-10-21 03:42:11,078 INFO  [main] index.IndexCollection (IndexCollection.java:879) - skipped:                0\n",
            "2020-10-21 03:42:11,078 INFO  [main] index.IndexCollection (IndexCollection.java:880) - errors:                 0\n",
            "2020-10-21 03:42:11,096 INFO  [main] index.IndexCollection (IndexCollection.java:883) - Total 16,987,130 documents indexed in 00:09:37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG6IJXTDpn1t"
      },
      "source": [
        "**Sample Testing of the indexing done by the Pyserini**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boUqTKXiDYYm",
        "outputId": "c0e55eac-60fc-4d85-a880-cfcdede74881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "from pyserini.search import SimpleSearcher\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "\n",
        "searcher = SimpleSearcher('pyserini/indexes/lucene-index-qasc')\n",
        "\n",
        "searcher.set_bm25(1.2, 0.75)\n",
        "\n",
        "hits = searcher.search(\"The dog is sleeping\")\n",
        "\n",
        "# Print the first 10 hits:\n",
        "for hit in hits:\n",
        "    print(f'{hit.docid:15} {hit.score:.5f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1339157         8.06910\n",
            "3656512         8.06910\n",
            "4791219         8.06910\n",
            "12316289        7.82960\n",
            "3322835         7.68200\n",
            "1283510         7.53660\n",
            "8128791         7.53660\n",
            "8479497         7.53660\n",
            "9651509         7.53660\n",
            "13082459        7.07000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z0aRhzVUEzs",
        "outputId": "25ffb760-ca82-4e14-ab2b-98a9a4ad0c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "d=searcher.doc(str(hits[0].docid))\n",
        "print(d.raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Document.raw of <pyserini.search._base.Document object at 0x7f7b675a0278>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO55GYg6YIWr",
        "outputId": "e3438135-7423-4675-c0e9-2fccd410d461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(hits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<io.anserini.search.SimpleSearcher$Result at 0x7f7bf64b8518 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0898 at 0x7f7c1acbeb50>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551518 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0900 at 0x7f7c1acbe5f0>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551200 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0908 at 0x7f7c1acbef50>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551468 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0950 at 0x7f7c1acbec30>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551410 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0968 at 0x7f7c1acbebf0>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b675513b8 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0970 at 0x7f7c1acbe650>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551360 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca0978 at 0x7f7c1acbefd0>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551258 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca1410 at 0x7f7c1acbed90>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b67551308 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca1418 at 0x7f7c1acbe770>>, <io.anserini.search.SimpleSearcher$Result at 0x7f7b675511a8 jclass=io/anserini/search/SimpleSearcher$Result jself=<LocalRef obj=0x2ca1420 at 0x7f7c1acbeeb0>>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekg1fnD9FcMG"
      },
      "source": [
        "R=[]\n",
        "doc = searcher.doc(str(hits[0].docid))\n",
        "R.append(doc.raw())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3uEjt4MFmgU",
        "outputId": "1965c7fc-5db1-4462-e68e-8f8b19aa376f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import json \n",
        "\n",
        "!pip install jsonlines\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.15.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COsLlhhG1cFJ"
      },
      "source": [
        "# **Getting the Input QASC DATASET to proper Format**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svxdYlykol2N"
      },
      "source": [
        "#Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzuUyiC7odKf"
      },
      "source": [
        "import jsonlines\n",
        "id=[]  #done\n",
        "question=[]   #done\n",
        "label_list=[] #done\n",
        "A=[] #done\n",
        "B=[] #done\n",
        "C=[] #done\n",
        "D=[] #done\n",
        "E=[] #done\n",
        "F=[] #done\n",
        "G=[] #done\n",
        "H=[] #done\n",
        "Actual_Answer=[] #done\n",
        "fact_1=[] #done\n",
        "fact_2=[] #done\n",
        "fact_combined=[] #done\n",
        "with jsonlines.open('/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/train.jsonl') as f:\n",
        "\n",
        "    for line in f.iter():\n",
        "      question.append(line['question']['stem'])\n",
        "      label_list.append(line['question']['choices'])\n",
        "      id.append(line[\"id\"])\n",
        "      Actual_Answer.append(line['answerKey'])\n",
        "      fact_1.append(line['fact1'])\n",
        "      fact_2.append(line['fact2'])\n",
        "      fact_combined.append(line['combinedfact'])\n",
        "\n",
        "#extracting label options into their list\n",
        "for i in range(0,len(label_list)):\n",
        "  for j in range(0,8):\n",
        "    if(label_list[i][j]['label']==\"A\"):\n",
        "      A.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"B\"):\n",
        "      B.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"C\"):\n",
        "      C.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"D\"):\n",
        "      D.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"E\"):\n",
        "      E.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"F\"):\n",
        "      F.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"G\"):\n",
        "      G.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"H\"):\n",
        "      H.append(label_list[i][j]['text'])\n",
        "for i in range(0,len(Actual_Answer)):\n",
        "  if(Actual_Answer[i]==\"A\"):\n",
        "    Actual_Answer[i]=A[i] \n",
        "  if(Actual_Answer[i]==\"B\"):\n",
        "    Actual_Answer[i]=B[i] \n",
        "  if(Actual_Answer[i]==\"C\"):\n",
        "    Actual_Answer[i]=C[i] \n",
        "  if(Actual_Answer[i]==\"D\"):\n",
        "    Actual_Answer[i]=D[i] \n",
        "  if(Actual_Answer[i]==\"E\"):\n",
        "    Actual_Answer[i]=E[i] \n",
        "  if(Actual_Answer[i]==\"F\"):\n",
        "    Actual_Answer[i]=F[i] \n",
        "  if(Actual_Answer[i]==\"G\"):\n",
        "    Actual_Answer[i]=G[i] \n",
        "  if(Actual_Answer[i]==\"H\"):\n",
        "    Actual_Answer[i]=H[i]  \n",
        "      \n",
        "  \n",
        "merged_list = tuple(zip(id,question,Actual_Answer,A,B,C,D,E,F,G,H,fact_1,fact_2,fact_combined))\n",
        "training_data=pd.DataFrame(merged_list,columns=['ID','Question','Actual Answer','A','B','C','D','E','F','G','H','Fact 1','Fact 2','Fact Combined'])\n",
        "#pd.read_json (r'/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/train.jsonl')\n",
        "\n",
        "##############################################################################\n",
        "answer_queries=[]\n",
        "for i in range(0,len(A)):\n",
        "    answer_queries.append(A[i])\n",
        "    answer_queries.append(B[i])\n",
        "    answer_queries.append(C[i])\n",
        "    answer_queries.append(D[i])\n",
        "    answer_queries.append(E[i])\n",
        "    answer_queries.append(F[i])\n",
        "    answer_queries.append(G[i])\n",
        "    answer_queries.append(H[i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mzGZiA122v9"
      },
      "source": [
        "**Extracting Sentences from corpus for each answer candidate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0wmT8JeeiJ3"
      },
      "source": [
        "###########################################################################\n",
        "from pyserini.search import SimpleSearcher\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "\n",
        "searcher = SimpleSearcher('pyserini/indexes/lucene-index-qasc')\n",
        "\n",
        "searcher.set_bm25(1.2, 0.75)\n",
        "answer_candidates=[]\n",
        "for i in range(0,len(answer_queries)):\n",
        "  # print(i)\n",
        "  hits = searcher.search(answer_queries[i])\n",
        "  if(len(hits)!=0):\n",
        "    doc = searcher.doc(str(hits[0].docid))\n",
        "    answer_candidates.append(doc.raw())\n",
        "  else:\n",
        "    answer_candidates.append(\"\")\n",
        "  # print(answer_candidates[i])\n",
        "############################################################################\n",
        "question_actaual_answer_dataframe=[]\n",
        "answer_candidates_dataframe=[]\n",
        "for i in range(0,len(question)):\n",
        "  answer_candidates_dataframe.append(fact_1[i])\n",
        "  answer_candidates_dataframe.append(fact_2[i])\n",
        "  answer_candidates_dataframe.append(fact_combined[i])\n",
        "  question_actaual_answer_dataframe.append(question[i]+\" \"+Actual_Answer[i])\n",
        "  question_actaual_answer_dataframe.append(question[i]+\" \"+Actual_Answer[i])\n",
        "  question_actaual_answer_dataframe.append(question[i]+\" \"+Actual_Answer[i])\n",
        "  for j in range(i*8,i*8+8):\n",
        "    answer_candidates_dataframe.append(answer_candidates[j])\n",
        "    question_actaual_answer_dataframe.append(question[i]+\" \"+Actual_Answer[i])\n",
        "###############################################################################\n",
        "QASC_df=pd.DataFrame({'Query':question_actaual_answer_dataframe,'Sentence':answer_candidates_dataframe})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMrGijGU2LjD"
      },
      "source": [
        "##############################################################################\n",
        "QASC_df.to_csv('/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/train_df.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf1ixv8jorqs"
      },
      "source": [
        "## DEV Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEwL13hUmxeQ"
      },
      "source": [
        "import jsonlines\n",
        "id=[]  #done\n",
        "question=[]   #done\n",
        "label_list=[] #done\n",
        "A=[] #done\n",
        "B=[] #done\n",
        "C=[] #done\n",
        "D=[] #done\n",
        "E=[] #done\n",
        "F=[] #done\n",
        "G=[] #done\n",
        "H=[] #done\n",
        "Actual_Answer=[] #done\n",
        "fact_1=[] #done\n",
        "fact_2=[] #done\n",
        "fact_combined=[] #done\n",
        "with jsonlines.open('/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/dev.jsonl') as f:\n",
        "\n",
        "    for line in f.iter():\n",
        "      question.append(line['question']['stem'])\n",
        "      label_list.append(line['question']['choices'])\n",
        "      id.append(line[\"id\"])\n",
        "      Actual_Answer.append(line['answerKey'])\n",
        "      fact_1.append(line['fact1'])\n",
        "      fact_2.append(line['fact2'])\n",
        "      fact_combined.append(line['combinedfact'])\n",
        "\n",
        "#extracting label options into their list\n",
        "for i in range(0,len(label_list)):\n",
        "  for j in range(0,8):\n",
        "    if(label_list[i][j]['label']==\"A\"):\n",
        "      A.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"B\"):\n",
        "      B.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"C\"):\n",
        "      C.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"D\"):\n",
        "      D.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"E\"):\n",
        "      E.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"F\"):\n",
        "      F.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"G\"):\n",
        "      G.append(label_list[i][j]['text'])\n",
        "      \n",
        "    if(label_list[i][j]['label']==\"H\"):\n",
        "      H.append(label_list[i][j]['text'])\n",
        "for i in range(0,len(Actual_Answer)):\n",
        "  if(Actual_Answer[i]==\"A\"):\n",
        "    Actual_Answer[i]=A[i] \n",
        "  if(Actual_Answer[i]==\"B\"):\n",
        "    Actual_Answer[i]=B[i] \n",
        "  if(Actual_Answer[i]==\"C\"):\n",
        "    Actual_Answer[i]=C[i] \n",
        "  if(Actual_Answer[i]==\"D\"):\n",
        "    Actual_Answer[i]=D[i] \n",
        "  if(Actual_Answer[i]==\"E\"):\n",
        "    Actual_Answer[i]=E[i] \n",
        "  if(Actual_Answer[i]==\"F\"):\n",
        "    Actual_Answer[i]=F[i] \n",
        "  if(Actual_Answer[i]==\"G\"):\n",
        "    Actual_Answer[i]=G[i] \n",
        "  if(Actual_Answer[i]==\"H\"):\n",
        "    Actual_Answer[i]=H[i]  \n",
        "      \n",
        "  \n",
        "merged_list = tuple(zip(id,question,Actual_Answer,A,B,C,D,E,F,G,H,fact_1,fact_2,fact_combined))\n",
        "training_data=pd.DataFrame(merged_list,columns=['ID','Question','Actual Answer','A','B','C','D','E','F','G','H','Fact 1','Fact 2','Fact Combined'])\n",
        "#pd.read_json (r'/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/train.jsonl')\n",
        "\n",
        "##############################################################################\n",
        "answer_queries=[]\n",
        "for i in range(0,len(A)):\n",
        "    answer_queries.append(A[i])\n",
        "    answer_queries.append(B[i])\n",
        "    answer_queries.append(C[i])\n",
        "    answer_queries.append(D[i])\n",
        "    answer_queries.append(E[i])\n",
        "    answer_queries.append(F[i])\n",
        "    answer_queries.append(G[i])\n",
        "    answer_queries.append(H[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47TXJuwP2uzb"
      },
      "source": [
        "# Extracting Sentences from corpus for each answer candidate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMklwqBz2Rip"
      },
      "source": [
        "###########################################################################\n",
        "from pyserini.search import SimpleSearcher\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "\n",
        "searcher = SimpleSearcher('pyserini/indexes/lucene-index-qasc')\n",
        "\n",
        "searcher.set_bm25(1.2, 0.75)\n",
        "answer_candidates=[]\n",
        "for i in range(0,len(answer_queries)):\n",
        "  # print(i)\n",
        "  hits = searcher.search(answer_queries[i])\n",
        "  if(len(hits)!=0):\n",
        "    doc = searcher.doc(str(hits[0].docid))\n",
        "    answer_candidates.append(doc.raw())\n",
        "  else:\n",
        "    answer_candidates.append(\"\")\n",
        "  # print(answer_candidates[i])\n",
        "############################################################################\n",
        "question_actaual_answer_dataframe=[]\n",
        "answer_candidates_dataframe=[]\n",
        "for i in range(0,len(question)):\n",
        "  answer_candidates_dataframe.append(fact_1[i])\n",
        "  answer_candidates_dataframe.append(fact_2[i])\n",
        "  answer_candidates_dataframe.append(fact_combined[i])\n",
        "  question_actaual_answer_dataframe.append(question[i]+\" \"+fact_combined[i])\n",
        "  question_actaual_answer_dataframe.append(question[i]+\" \"+Actual_Answer[i])\n",
        "  question_actaual_answer_dataframe.append(question[i]+\" \"+Actual_Answer[i][i])\n",
        "  for j in range(i*8,i*8+8):\n",
        "    answer_candidates_dataframe.append(answer_candidates[j])\n",
        "    question_actaual_answer_dataframe.append(fact_combined[i])\n",
        "###############################################################################\n",
        "QASC_df=pd.DataFrame({'Query':question_actaual_answer_dataframe,'Sentence':answer_candidates_dataframe})\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rPl_3en2pze"
      },
      "source": [
        "# Exporting the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqQAfPCe2T4G"
      },
      "source": [
        "##############################################################################\n",
        "QASC_df.to_csv('/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/dev_df_factc.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erwHnuOC8xTn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}